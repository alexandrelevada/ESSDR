{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxmCLgKh9NXDijBsPoOiCi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**Semi-Supervised Metric Learning with Information-Theoretic Distances: A Dimensionality Reduction Based Approach**\n","\n","Alaor Cervati Neto, Alexandre L. M. Levada\n","\n","To run this notebook online, click here:\n","\n","https://colab.research.google.com/drive/1tjzDcitJb2fjT7yiBxFmMBQd5kWcJX7z?usp=sharing\n"],"metadata":{"id":"m_K59JY2l0Pv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5IMW__Clzq9","executionInfo":{"status":"ok","timestamp":1682912982592,"user_tz":180,"elapsed":40491836,"user":{"displayName":"Alexandre Levada","userId":"02637985322644396645"}},"outputId":"3d323393-d9d7-4f38-8f01-da54f9dae152"},"outputs":[{"output_type":"stream","name":"stdout","text":["N =  2000\n","M =  76\n","C = 10\n","K = 45\n","\n","PCA\n","\n","ISOMAP\n","\n","LLE\n","\n","Laplacian Eigenmaps\n","\n","LDA\n","\n","==================================== SSDR =====================================\n","i = 0\n","i = 1\n","i = 2\n","i = 3\n","i = 4\n","i = 5\n","i = 6\n","i = 7\n","i = 8\n","i = 9\n","i = 10\n","i = 11\n","i = 12\n","i = 13\n","i = 14\n","================================== Entropic SSDR =======================================\n","k = 2\n","k = 3\n","k = 4\n","k = 5\n","k = 6\n","k = 7\n","k = 8\n","k = 9\n","k = 10\n","k = 11\n","k = 12\n","k = 13\n","k = 14\n","k = 15\n","k = 16\n","k = 17\n","k = 18\n","k = 19\n","k = 20\n","=============================== Entropic SSDR MST ====================================\n","k = 2\n","k = 3\n","k = 4\n","k = 5\n","k = 6\n","k = 7\n","k = 8\n","k = 9\n","k = 10\n","k = 11\n","k = 12\n","k = 13\n","k = 14\n","k = 15\n","k = 16\n","k = 17\n","k = 18\n","k = 19\n","k = 20\n","========== RESULTS ==========\n","\n","PCA SC: 0.000658\n","PCA acc: 0.434000\n","\n","ISOMAP SC: 0.011321\n","ISOMAP acc: 0.430000\n","\n","LLE SC: -0.053948\n","LLE acc: 0.452000\n","\n","Laplacian Eigenmaps SC: -0.003160\n","Laplacian Eigenmaps acc: 0.447000\n","\n","LDA SC (supervised): 0.242042\n","LDA acc (supervised): 0.689000\n","\n","SSDR SC: -0.053971\n","SSDR acc: 0.396667\n","\n","Entropic SSDR SC: 0.029699\n","Entropic SSDR acc: 0.472000\n","K* = 12\n","\n","Entropic SSDR MST SC: 0.181995\n","Entropic SSDR MST acc: 0.655000\n","K* = 20\n","\n"]}],"source":["# Imports\n","import sys\n","import time\n","import warnings\n","import sklearn.datasets as skdata\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import itertools\n","import random\n","import networkx as nx\n","from numpy import log\n","from numpy import trace\n","from numpy import dot\n","from scipy import stats\n","from numpy.linalg import det\n","from scipy.linalg import eigh\n","from numpy.linalg import inv\n","from numpy.linalg import cond\n","from numpy import eye\n","from sklearn import preprocessing\n","from sklearn import metrics\n","import sklearn.neighbors as sknn\n","from sklearn.mixture import GaussianMixture\n","from sklearn.manifold import Isomap\n","from sklearn.manifold import LocallyLinearEmbedding\n","from sklearn.manifold import SpectralEmbedding\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from networkx.algorithms.centrality import edge_betweenness_centrality\n","\n","# To avoid unnecessary warning messages\n","warnings.simplefilter(action='ignore')\n","\n","# PCA implementation\n","def myPCA(dados, d):\n","    # Eigenvalues and eigenvectors of the covariance matrix\n","    v, w = np.linalg.eig(np.cov(dados.T))\n","    # Sort the eigenvalues\n","    ordem = v.argsort()\n","    # Select the d eigenvectors associated to the d largest eigenvalues\n","    maiores_autovetores = w[:, ordem[-d:]]\n","    # Projection matrix\n","    Wpca = maiores_autovetores\n","    # Linear projection into the 2D subspace\n","    novos_dados = np.dot(Wpca.T, dados.T)\n","    return novos_dados\n","\n","# Bhattacharyya divergence\n","def Bhattacharyya(mu1, mu2, cov1, cov2):\n","    m = len(mu1)\n","    Sigma = (cov1 + cov2)/2\n","    # If covariance matrices are ill-conditioned\n","    if np.linalg.cond(cov1) > 1/sys.float_info.epsilon:\n","        cov1 = cov1 + np.diag(0.001*np.ones(m))\n","    if np.linalg.cond(cov2) > 1/sys.float_info.epsilon:\n","        cov2 = cov2 + np.diag(0.001*np.ones(m))\n","    if np.linalg.cond(Sigma) > 1/sys.float_info.epsilon:\n","        Sigma = Sigma + np.diag(0.001*np.ones(m))\n","    dM = (1/8)*(mu1-mu2).T.dot(inv(Sigma)).dot(mu1-mu2)\n","    dD = 0.5*log(det(Sigma)/np.sqrt(det(cov1)*det(cov2)))\n","    dB = dM + dD\n","    return dB\n","\n","# KL-divergence\n","def divergenciaKL(mu1, mu2, cov1, cov2):\n","    m = len(mu1)\n","    # If covariance matrices are ill-conditioned\n","    if np.linalg.cond(cov1) > 1/sys.float_info.epsilon:\n","        cov1 = cov1 + np.diag(0.001*np.ones(m))\n","    if np.linalg.cond(cov2) > 1/sys.float_info.epsilon:\n","        cov2 = cov2 + np.diag(0.001*np.ones(m))\n","    dM1 = 0.5*(mu2-mu1).T.dot(inv(cov2)).dot(mu2-mu1)\n","    dM2 = 0.5*(mu1-mu2).T.dot(inv(cov1)).dot(mu1-mu2)\n","    dTr = 0.5*trace(dot(inv(cov1), cov2) + dot(inv(cov2), cov1))\n","    dKL = 0.5*(dTr + dM1 + dM2 - m)\n","    return dKL\n","\n","# Cauchy-Schwarz divergence\n","def CauchySchwarz(mu1, mu2, cov1, cov2):\n","    m = len(mu1)\n","    # If covariance matrices are ill-conditioned\n","    if np.linalg.cond(cov1) > 1/sys.float_info.epsilon:\n","        cov1 = cov1 + np.diag(0.001*np.ones(m))\n","    if np.linalg.cond(cov2) > 1/sys.float_info.epsilon:\n","        cov2 = cov2 + np.diag(0.001*np.ones(m))\n","    Sigma_inv = inv(cov1) + inv(cov2)\n","    if np.linalg.cond(Sigma_inv) > 1/sys.float_info.epsilon:\n","        Sigma_inv = Sigma_inv + np.diag(0.001*np.ones(m))\n","    T1 = (1/4)*log(det(cov1/2)) + 0.5*(mu1).T.dot(inv(cov1)).dot(mu1)\n","    T2 = (1/4)*log(det(cov2/2)) + 0.5*(mu2).T.dot(inv(cov2)).dot(mu2)\n","    T3 = 0.5*(log(det(inv(cov1) + inv(cov2))))\n","    T4 = 0.5*dot((dot(inv(cov1), mu1) + dot(inv(cov2), mu2)).T, dot(inv(Sigma_inv), (dot(inv(cov1), mu1) + dot(inv(cov2), mu2))))\n","    dCS = T1 + T2 + T3 - T4    \n","    return dCS\n","\n","# Semi-supervised dimensionality reduction (regular)\n","def SSDR(dados, target, perc, alpha, beta, d):\n","    # Number of samples\n","    n = dados.shape[0]\n","    # Number of features\n","    m = dados.shape[1]\n","    # Pairwise constraints\n","    x = list(range(0, n))\n","    pares = list(itertools.combinations(x, 2))\n","    # Select percentage perc of the total pairs\n","    num = round(perc*n)    # 0 < perc < 1\n","    L = random.sample(pares, num)\n","    # Number of elements in C and M \n","    NC = 0\n","    NM = 0\n","    for i in range(len(L)):\n","        if target[L[i][0]] == target[L[i][1]]:\n","            NM += 1  \n","        else:\n","            NC += 1\n","    # Build the complete graph\n","    S = np.zeros((n, n))\n","    for i in range(n):\n","        for j in range(n):\n","            if (i, j) in L:\n","                if target[i] == target[j]:  # Must-Link constraint\n","                    S[i, j] = (1/n**2) - beta/NM \n","                else:                       # Cannot-link constraint\n","                    S[i, j] = (1/n**2) + alpha/NC\n","            else:\n","                S[i, j] = (1/n**2) \n","    # Degree matrix D and Laplacian L\n","    D = np.diag(S.sum(1))   \n","    L = D - S\n","    # Final matrix\n","    X = dados.T\n","    M = np.dot(np.dot(X, L), X.T)\n","    lambdas, alphas = eigh(M)  \n","    ordem = lambdas.argsort()\n","    # Select the d eigenvectors associated to the d largest eigenvalues\n","    maiores_autovetores = alphas[:, ordem[-d:]]\n","    # Projection matrix\n","    Wssdr = maiores_autovetores \n","    # Project data\n","    output = np.dot(Wssdr.T, X)\n","    return output\n","\n","# Entropic SSDR (proposed method - variation 1)\n","def Entropic_SSDR(dados, target, dist, k, perc, alpha, beta, d):\n","    # Number of samples\n","    n = dados.shape[0]\n","    # Number of features\n","    m = dados.shape[1]\n","    # Define the KNN graph\n","    knnGraph = sknn.kneighbors_graph(dados, n_neighbors=k, mode='connectivity')\n","    W = knnGraph.toarray()\n","    # Generate the pairwise constraints\n","    x = list(range(0, n))\n","    pares = list(itertools.combinations(x, 2))\n","    # Select percentage perc of the total pairs\n","    num = round(perc*n)    # 0 < perc < 1\n","    L = random.sample(pares, num)\n","    # Number of elements in C and M \n","    NC = 0\n","    NM = 0\n","    for i in range(len(L)):\n","        if target[L[i][0]] == target[L[i][1]]:\n","            NM += 1  \n","        else:\n","            NC += 1\n","    # Build the complete graph\n","    if NC == 0:\n","        NC += 1\n","    if NM == 0:\n","        NM += 1\n","    S = np.zeros((n, n))\n","    for i in range(n):\n","        for j in range(n):            \n","            if (i, j) in L:\n","                # Compute the divergence between patches\n","                # Extract patch 1\n","                vizinhos = W[i, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:    # treat isolated points\n","                    media_i = X[i, :]\n","                    matriz_covariancias_i = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_i = amostras.mean(0)\n","                    matriz_covariancias_i = np.cov(amostras.T)\n","                # Extract patch 2    \n","                vizinhos = W[j, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:    # treat isolated points\n","                    media_j = X[j, :]\n","                    matriz_covariancias_j = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_j = amostras.mean(0)\n","                    matriz_covariancias_j = np.cov(amostras.T)\n","                # Select the stochastic divergence to compute\n","                if dist == 'KL':\n","                    DKL_ij = divergenciaKL(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                    DKL_ji = divergenciaKL(media_j, media_i, matriz_covariancias_j, matriz_covariancias_i)\n","                    distance = 0.5*(DKL_ij + DKL_ji)\n","                elif dist == 'BHAT':\n","                    distance = Bhattacharyya(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                elif dist == 'CS':\n","                    distance = CauchySchwarz(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                # If distance is infinite, define a upper bound\n","                if np.isinf(distance):\n","                    distance = 100\n","                # If distance is NaN, replace by a small value\n","                elif np.isnan(distance):\n","                    distance = 0.001\n","                # Compute the values of the matrix S\n","                if target[i] == target[j]:  # Must-Link constraint\n","                    S[i, j] = (1/n**2) - (1/NM)*distance*beta\n","                else:   # Cannot-link constraint\n","                    S[i, j] = (1/n**2) + (1/NC)*distance*alpha\n","            else:\n","                S[i, j] = (1/n**2)\n","    # Degree matrix D and Laplacian L\n","    D = np.diag(S.sum(1))   \n","    L = D - S\n","    # Final matrix\n","    X = dados.T\n","    M = np.dot(np.dot(X, L), X.T)\n","    lambdas, alphas = eigh(M)  \n","    ordem = lambdas.argsort()\n","    # Select the d eigenvectors associated to the d largest eigenvalues\n","    maiores_autovetores = alphas[:, ordem[-d:]]\n","    # Projection matrix\n","    Wssdr = maiores_autovetores \n","    # Project data\n","    output = np.dot(Wssdr.T, X)\n","    return output\n","\n","# MST-ESSDR: Entropic SSDR with MST for pairwise constraints selection\n","def Entropic_SSDR_MST(dados, target, dist, k, perc, alpha, beta, d):\n","    # Number of samples\n","    n = dados.shape[0]\n","    # Number of features\n","    m = dados.shape[1]\n","    # Number of classes\n","    c = len(np.unique(target))\n","    # Labels estimation with GMM model\n","    gmm_labels = GaussianMixture(n_components=c, random_state=0).fit_predict(dados)\n","    # Build the KNN graph\n","    knnGraph = sknn.kneighbors_graph(dados, n_neighbors=k, mode='distance')\n","    W = knnGraph.toarray()\n","    # Generate pairwise constraints\n","    x = list(range(0, n))\n","    pares = list(itertools.combinations(x, 2))\n","    # Select percentage perc of the total pairs\n","    num = round(perc*n)    # 0 < perc < 1\n","    L = random.sample(pares, num)\n","    # Pairwise constraints using MST\n","    G = nx.from_numpy_array(W)\n","    W_mst = nx.minimum_spanning_tree(G)\n","    mst_edges = W_mst.edges()\n","    # Find the number of elements in C and M\n","    NC = 0\n","    NM = 0\n","    for i in range(len(L)):\n","        if target[L[i][0]] == target[L[i][1]]:\n","            NM += 1  \n","        else:\n","            NC += 1\n","    if NC == 0:\n","        NC = 1\n","    if NM == 0:\n","        NM = 1\n","    # Build the complete graph (S)\n","    S = np.zeros((n, n))\n","    for i in range(n):\n","        for j in range(n):\n","            if (i, j) in L:   # use the real labels\n","                # Extract the patch 1\n","                vizinhos = W[i, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:   # treat isolated points\n","                    media_i = dados[i, :]\n","                    matriz_covariancias_i = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_i = amostras.mean(0)\n","                    matriz_covariancias_i = np.cov(amostras.T)\n","                # Extract the patch 2\n","                vizinhos = W[j, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:   # treat isolated points\n","                    media_j = dados[j, :]\n","                    matriz_covariancias_j = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_j = amostras.mean(0)\n","                    matriz_covariancias_j = np.cov(amostras.T)\n","                # Select the stochastic divergence\n","                if dist == 'KL':\n","                    DKL_ij = divergenciaKL(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                    DKL_ji = divergenciaKL(media_j, media_i, matriz_covariancias_j, matriz_covariancias_i)\n","                    distance = 0.5*(DKL_ij + DKL_ji)\n","                elif dist == 'BHAT':\n","                    distance = Bhattacharyya(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                elif dist == 'CS':\n","                    distance = CauchySchwarz(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                # If distance is infinite, define a upper bound\n","                if np.isinf(distance):\n","                    distance = 100\n","                # If distance is NaN, replace by a small value\n","                elif np.isnan(distance):\n","                    distance = 0.001\n","                # Compute the values of the matrix S\n","                if target[i] == target[j]:    # Must-Link constraint\n","                    S[i, j] = (1/n**2) - (1/NM)*distance*beta\n","                else:                         # Cannot-link constraint\n","                    S[i, j] = (1/n**2) + (1/NC)*distance*alpha\n","            elif (i, j) in mst_edges:   # Use the estimated labels if we are in a MST edge\n","                # Extract the patch 1\n","                vizinhos = W[i, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:   # treat isolated points\n","                    media_i = dados[i, :]\n","                    matriz_covariancias_i = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_i = amostras.mean(0)\n","                    matriz_covariancias_i = np.cov(amostras.T)\n","                # Extract the patch 2\n","                vizinhos = W[j, :]\n","                indices = vizinhos.nonzero()[0]\n","                if len(indices) < 2:   # treat isolated points\n","                    media_j = dados[j, :]\n","                    matriz_covariancias_j = np.eye(dados.shape[1])\n","                else:\n","                    amostras = dados[indices]\n","                    media_j = amostras.mean(0)\n","                    matriz_covariancias_j = np.cov(amostras.T)\n","                # Select the stochastic divergence\n","                if dist == 'KL':\n","                    DKL_ij = divergenciaKL(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                    DKL_ji = divergenciaKL(media_j, media_i, matriz_covariancias_j, matriz_covariancias_i)\n","                    distance = 0.5*(DKL_ij + DKL_ji)\n","                elif dist == 'BHAT':\n","                    distance = Bhattacharyya(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                elif dist == 'CS':\n","                    distance = CauchySchwarz(media_i, media_j, matriz_covariancias_i, matriz_covariancias_j)\n","                # If distance is infinite, define a upper bound\n","                if np.isinf(distance):\n","                    distance = 100\n","                # If distance is NaN, replace by a small value\n","                elif np.isnan(distance):\n","                    distance = 0.001\n","                # Compute the values of the matrix S\n","                if gmm_labels[i] == gmm_labels[j]:    # Must-Link constraint\n","                    S[i, j] = (1/n**2) - (1/NM)*distance*beta\n","                else:                                 # Cannot-link constraint\n","                    S[i, j] = (1/n**2) + (1/NC)*distance*alpha\n","            else:\n","                S[i, j] = (1/n**2)\n","    # Degree matrix D and Laplacian L\n","    D = np.diag(S.sum(1))   \n","    L = D - S\n","    # Final matrix\n","    X = dados.T\n","    M = np.dot(np.dot(X, L), X.T)\n","    lambdas, alphas = eigh(M)  \n","    ordem = lambdas.argsort()\n","    # Select the d eigenvectors associated to the d largest eigenvalues\n","    maiores_autovetores = alphas[:, ordem[-d:]]\n","    # Projection matrix\n","    Wssdr = maiores_autovetores \n","    # Projeta data\n","    output = np.dot(Wssdr.T, X)\n","    return output\n","\n","'''\n"," Computes the Silhouette coefficient and the supervised classification\n"," accuracies for several classifiers: KNN, SVM, NB, DT, MPL, GPC and RFC\n"," dados: learned representation (output of a dimens. reduction - DR)\n"," target: ground-truth (data labels)\n"," '''\n","def Classification(dados, target, method):\n","    # print()\n","    # print('Supervised classification for %s features' %(method))\n","    # print()\n","    \n","    lista = []\n","\n","    # 50% for training and 50% for testing\n","    X_train, X_test, y_train, y_test = train_test_split(dados.real.T, target, test_size=.5, random_state=42)\n","\n","    # KNN\n","    neigh = KNeighborsClassifier(n_neighbors=7)\n","    neigh.fit(X_train, y_train) \n","    acc = neigh.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('KNN accuracy: ', acc)\n","\n","    # SMV\n","    svm = SVC(gamma='auto')\n","    svm.fit(X_train, y_train) \n","    acc = svm.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('SVM accuracy: ', acc)\n","\n","    # Naive Bayes\n","    nb = GaussianNB()\n","    nb.fit(X_train, y_train)\n","    acc = nb.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('NB accuracy: ', acc)\n","\n","    # Decision Tree\n","    dt = DecisionTreeClassifier(random_state=0)\n","    dt.fit(X_train, y_train)\n","    acc = dt.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('DT accuracy: ', acc)\n","\n","    # MPL classifier\n","    mpl = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', max_iter=5000)\n","    mpl.fit(X_train, y_train)\n","    acc = mpl.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('MPL accuracy: ', acc)\n","\n","    # Gaussian Process\n","    gpc = GaussianProcessClassifier()\n","    gpc.fit(X_train, y_train)\n","    acc = gpc.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('GPC accuracy: ', acc)\n","\n","    # Random Forest Classifier\n","    rfc = RandomForestClassifier()\n","    rfc.fit(X_train, y_train)\n","    acc = rfc.score(X_test, y_test)\n","    lista.append(acc)\n","    #print('RFC accuracy: ', acc)\n","\n","    # Computes the Silhoutte coefficient\n","    sc = metrics.silhouette_score(dados.real.T, target, metric='euclidean')\n","    ch = metrics.calinski_harabasz_score(dados.real.T, target)\n","    db = metrics.davies_bouldin_score(dados.real.T, target)\n","    #print('Silhouette coefficient: ', sc)\n","    #print('Calinski Harabasz: ', ch)\n","    #print('Davies Bouldin: ', db)\n","    \n","    # Computes the average accuracy\n","    average = sum(lista)/len(lista)\n","    maximo = max(lista)\n","\n","    #print('Average accuracy: ', average)\n","    #print('Maximum accuracy: ', maximo)\n","    #print()\n","\n","    #return [sc, average, ch, db]\n","    return [sc, maximo, ch, db]\n","\n","\n","# Plot scatterplots in 2D\n","def PlotaDados(dados, labels, metodo):\n","    nclass = len(np.unique(labels))\n","    if metodo == 'LDA':\n","        if nclass == 2:\n","            return -1\n","    # Convert labels to integers\n","    lista = []\n","    for x in labels:\n","        if x not in lista:  \n","            lista.append(x)     \n","    # Map labels to integers\n","    rotulos = []\n","    for x in labels:  \n","        for i in range(len(lista)):\n","            if x == lista[i]:  \n","                rotulos.append(i)\n","    # Convert to array\n","    rotulos = np.array(rotulos)\n","    # Select colors\n","    if nclass > 11:\n","        cores = ['black', 'gray', 'silver', 'whitesmoke', 'rosybrown', 'firebrick', 'red', 'darksalmon', 'sienna', 'sandybrown', 'bisque', 'tan', 'moccasin', 'floralwhite', 'gold', 'darkkhaki', 'lightgoldenrodyellow', 'olivedrab', 'chartreuse', 'palegreen', 'darkgreen', 'seagreen', 'mediumspringgreen', 'lightseagreen', 'paleturquoise', 'darkcyan', 'darkturquoise', 'deepskyblue', 'aliceblue', 'slategray', 'royalblue', 'navy', 'blue', 'mediumpurple', 'darkorchid', 'plum', 'm', 'mediumvioletred', 'palevioletred']\n","        np.random.shuffle(cores)\n","    else:\n","        cores = ['blue', 'red', 'cyan', 'black', 'orange', 'magenta', 'green', 'darkkhaki', 'brown', 'purple', 'salmon']\n","    # Create figure\n","    plt.figure(1)\n","    for i in range(nclass):\n","        indices = np.where(rotulos==i)[0]\n","        cor = cores[i]\n","        plt.scatter(dados[indices, 0], dados[indices, 1], c=cor, marker='*')\n","    # Save file    \n","    nome_arquivo = metodo + '.png'\n","    plt.title(metodo+' clusters')\n","    plt.savefig(nome_arquivo)\n","    plt.close()\n","\n","#%%%%%%%%%%%%%%%%%%%%  Data loading\n","# OpenML datasets\n","# Select one!\n","#X = skdata.load_iris()\n","#X = skdata.fetch_openml(name='veteran', version=2) \n","#X = skdata.fetch_openml(name='aids', version=1) \n","#X = skdata.fetch_openml(name='bolts', version=2) \n","#X = skdata.fetch_openml(name='threeOf9', version=1) \n","#X = skdata.fetch_openml(name='balance-scale', version=1) \n","#X = skdata.fetch_openml(name='user-knowledge', version=1)    \n","#X = skdata.fetch_openml(name='monks-problems-1', version=1)            \n","#X = skdata.fetch_openml(name='planning-relax', version=1)              \n","#X = skdata.fetch_openml(name='prnn_crabs', version=1)\n","#X = skdata.fetch_openml(name='fri_c0_500_10', version=2)\n","#X = skdata.fetch_openml(name='diggle_table_a2', version=1)\n","#X = skdata.fetch_openml(name='pwLinear', version=2)\n","#X = skdata.fetch_openml(name='chscase_census5', version=2)\n","#X = skdata.fetch_openml(name='blogger', version=1)\n","#X = skdata.fetch_openml(name='qualitative-bankruptcy', version=1)\n","#X = skdata.fetch_openml(name='KungChi3', version=1)\n","#X = skdata.fetch_openml(name='MegaWatt1', version=1)\n","#X = skdata.fetch_openml(name='diabetes_numeric', version=2)\n","X = skdata.fetch_openml(name='mfeat-fourier', version=1) \n","#X = skdata.fetch_openml(name='corral', version=1)\n","#X = skdata.fetch_openml(name='xd6', version=1)\n","\n","dados = X['data']\n","target = X['target']  \n","\n","n = dados.shape[0]\n","m = dados.shape[1]\n","c = len(np.unique(target))\n","nn = round(np.sqrt(n))\n","\n","print('N = ', n)\n","print('M = ', m)\n","print('C = %d' %c)\n","print('K = %d' %nn)\n","print()\n","\n","# Only for OpenML datasets\n","# Need to treat categorical data manually\n","if not isinstance(dados, np.ndarray):\n","    cat_cols = dados.select_dtypes(['category']).columns\n","    dados[cat_cols] = dados[cat_cols].apply(lambda x: x.cat.codes)\n","    dados = dados.to_numpy()\n","    target = target.to_numpy()\n","\n","# Data standardization (to deal with variables having different units/scales)\n","dados = preprocessing.scale(dados)\n","\n","#%%%%%%%%%%% Simple PCA\n","print('PCA\\n') \n","dados_pca = myPCA(dados, 2)\n","\n","#%%%%%%%%%%% ISOMAP\n","print('ISOMAP\\n')\n","model = Isomap(n_neighbors=nn, n_components=2)\n","dados_isomap = model.fit_transform(dados)\n","dados_isomap = dados_isomap.T\n","\n","#%%%%%%%%%%% LLE\n","print('LLE\\n')\n","model = LocallyLinearEmbedding(n_neighbors=nn, n_components=2)\n","dados_LLE = model.fit_transform(dados)\n","dados_LLE = dados_LLE.T\n","\n","#%%%%%%%%%%% Lap. Eig.\n","print('Laplacian Eigenmaps\\n')\n","model = SpectralEmbedding(n_neighbors=nn, n_components=2)\n","dados_Lap = model.fit_transform(dados)\n","dados_Lap = dados_Lap.T\n","\n","#%%%%%%%%%%% LDA\n","print('LDA\\n')\n","if c > 2:\n","    model = LinearDiscriminantAnalysis(n_components=2)\n","else:\n","    model = LinearDiscriminantAnalysis(n_components=1)\n","dados_lda = model.fit_transform(dados, target)\n","dados_lda = dados_lda.T\n","\n","#%%%%%%%%%%% Supervised classification\n","L_pca = Classification(dados_pca.real, target, 'PCA')\n","L_iso = Classification(dados_isomap, target, 'ISOMAP')\n","L_lle = Classification(dados_LLE, target, 'LLE')\n","L_lap = Classification(dados_Lap, target, 'Lap. Eig.')\n","L_lda = Classification(dados_lda, target, 'LDA')\n","\n","#%%%%%%%%%%%% SSDR\n","print('==================================== SSDR =====================================')\n","MAX = 15    # Number of executions\n","avg_ssdr = np.zeros(MAX)\n","sc_ssdr = np.zeros(MAX)\n","ch_ssdr = np.zeros(MAX)\n","db_ssdr = np.zeros(MAX)\n","for i in range(MAX):\n","    print('i = %d' %i)\n","    dados_ssdr = SSDR(dados, target, perc=0.1, alpha=1, beta=10, d=2)\n","    L_ssdr = Classification(dados_ssdr, target, 'SSDR')\n","    sc_ssdr[i] = L_ssdr[0]\n","    avg_ssdr[i] = L_ssdr[1]\n","    ch_ssdr[i] = L_ssdr[2]\n","    db_ssdr[i] = L_ssdr[3]\n","\n","#%%%%%%%%%%%% Entropic SSDR\n","print('================================== Entropic SSDR =======================================')\n","MAX = 15    # Number of executions\n","fim = min(21, n//3)     # Interval of values for K (number of neighbors)\n","lista_k = list(range(2, fim))\n","avg_essdr = np.zeros(MAX)\n","sc_essdr = np.zeros(MAX)\n","ch_essdr = np.zeros(MAX)\n","db_essdr = np.zeros(MAX)\n","acuracias_essdr = np.zeros(len(lista_k))\n","scs_essdr = np.zeros(len(lista_k))\n","chs_essdr = np.zeros(len(lista_k))\n","dbs_essdr = np.zeros(len(lista_k))\n","for k in lista_k:\n","    print('k = %d' %k)\n","    for i in range(MAX):\n","        dados_ent_ssdr = Entropic_SSDR(dados, target, dist='KL', k=k, perc=0.1, alpha=1, beta=10, d=2)\n","        L_ent_ssdr = Classification(dados_ent_ssdr, target, 'Entropic SSDR')\n","        sc_essdr[i] = L_ent_ssdr[0]\n","        avg_essdr[i] = L_ent_ssdr[1]\n","        ch_essdr[i] = L_ent_ssdr[2]\n","        db_essdr[i] = L_ent_ssdr[3]\n","    acuracias_essdr[k-2] = avg_essdr.max() \n","    scs_essdr[k-2] = sc_essdr.max()\n","    chs_essdr[k-2] = ch_essdr.max()\n","    dbs_essdr[k-2] = db_essdr.max()\n","\n","#%%%%%%%%%%%% Entropic SSDR MST\n","print('=============================== Entropic SSDR MST ====================================')\n","MAX = 15    # Number of executions\n","fim = min(21, n//3)   # Interval of values for K (number of neighbors)\n","lista_k = list(range(2, fim))\n","avg_essdr_mst = np.zeros(MAX)\n","sc_essdr_mst = np.zeros(MAX)\n","ch_essdr_mst = np.zeros(MAX)\n","db_essdr_mst = np.zeros(MAX)\n","acuracias_essdr_mst = np.zeros(len(lista_k))\n","scs_essdr_mst = np.zeros(len(lista_k))\n","chs_essdr_mst = np.zeros(len(lista_k))\n","dbs_essdr_mst = np.zeros(len(lista_k))\n","for k in lista_k:\n","    print('k = %d' %k)\n","    for i in range(MAX):\n","        dados_ent_ssdr_mst = Entropic_SSDR_MST(dados, target, dist='KL', k=k, perc=0.1, alpha=1, beta = 2, d=2)\n","        L_ent_ssdr_mst = Classification(dados_ent_ssdr_mst, target, 'Entropic SSDR MST')\n","        sc_essdr_mst[i] = L_ent_ssdr_mst[0]\n","        avg_essdr_mst[i] = L_ent_ssdr_mst[1]\n","        ch_essdr_mst[i] = L_ent_ssdr_mst[2]\n","        db_essdr_mst[i] = L_ent_ssdr_mst[3]\n","    acuracias_essdr_mst[k-2] = avg_essdr_mst.max()\n","    scs_essdr_mst[k-2] = sc_essdr_mst.max()\n","    chs_essdr_mst[k-2] = ch_essdr_mst.max()\n","    dbs_essdr_mst[k-2] = db_essdr_mst.max()    \n","\n","# Print results\n","print('========== RESULTS ==========')\n","print()\n","\n","print('PCA SC: %f' %L_pca[0])\n","print('PCA acc: %f' %L_pca[1])\n","print()\n","\n","print('ISOMAP SC: %f' %L_iso[0])\n","print('ISOMAP acc: %f' %L_iso[1])\n","print()\n","\n","print('LLE SC: %f' %L_lle[0])\n","print('LLE acc: %f' %L_lle[1])\n","print()\n","\n","print('Laplacian Eigenmaps SC: %f' %L_lap[0])\n","print('Laplacian Eigenmaps acc: %f' %L_lap[1])\n","print()\n","\n","print('LDA SC (supervised): %f' %L_lda[0])\n","print('LDA acc (supervised): %f' %L_lda[1])\n","print()\n","\n","print('SSDR SC: %f' %sc_ssdr.mean())\n","print('SSDR acc: %f' %avg_ssdr.mean())\n","print()\n","\n","print('Entropic SSDR SC: %f' %max(scs_essdr))\n","print('Entropic SSDR acc: %f' %max(acuracias_essdr))\n","print('K* = %d' %(acuracias_essdr.argmax()+2))\n","print()\n","\n","print('Entropic SSDR MST SC: %f' %max(scs_essdr_mst))\n","print('Entropic SSDR MST acc: %f' %max(acuracias_essdr_mst))\n","print('K* = %d' %(acuracias_essdr_mst.argmax()+2))\n","print()\n","\n","\n","# Plot data\n","PlotaDados(dados_pca.T, target, 'PCA')\n","PlotaDados(dados_isomap.T, target, 'ISOMAP')\n","PlotaDados(dados_LLE.T, target, 'LLE')\n","PlotaDados(dados_Lap.T, target, 'LAP')\n","PlotaDados(dados_lda.T, target, 'LDA')\n","PlotaDados(dados_ssdr.T, target, 'SSDR')\n","PlotaDados(dados_ent_ssdr.T, target, 'ENT SSDR')\n","PlotaDados(dados_ent_ssdr_mst.T, target, 'ENT SSDR MST')\n"]}]}